{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime, time\n",
    "import socket, sys, cv2, numpy, time\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import csv\n",
    "import datetime, time\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import threading\n",
    "import datetime\n",
    "import scipy\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isGreen(rgb):\n",
    "    r, g, b = rgb\n",
    "    if (g>r) and (g>b):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isBlue(rgb):\n",
    "    r, g, b = rgb\n",
    "    if (b>r) and (b>g):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isRed(rgb):\n",
    "    r, g, b = rgb\n",
    "    if (r>b) and (r>g):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHealths(im, coord_p1, coord_p2, right_end):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    #verde es #3CD400 == rgb(60,212,0) aproximadamente, isGreen comprueba que G sea el valor principal\n",
    "    for x in range(right_end+1):\n",
    "        if (isGreen(im.getpixel(coord_p1))):\n",
    "            p1 = p1 + 1\n",
    "        if (isGreen(im.getpixel(coord_p2))):\n",
    "            p2 = p2 + 1\n",
    "        coord_p1 = x, y = coord_p1[0]+1, coord_p1[1]\n",
    "        coord_p2 = x, y = coord_p2[0]+1, coord_p2[1]\n",
    "    return(p1,p2)\n",
    "\n",
    "def is_game_over(HP_1, HP_2, end_time):\n",
    "    if (HP_1 == 0):\n",
    "        return -1 #P1 LOOSES\n",
    "    elif (HP_2 == 0):\n",
    "        return 1 #P2 LOOSES\n",
    "    elif ((HP_1 == 0) and (HP_2 == 0)) or (datetime.datetime.now() > end_time):\n",
    "        return 2 #DRAW\n",
    "    return 0\n",
    "\n",
    "def is_game_over_neutral_endless(HP_1, HP_2):\n",
    "    if (HP_1 == 0):\n",
    "        return 1 #P1 LOOSES\n",
    "    elif (HP_2 == 0):\n",
    "        return 2 #P2 LOOSES\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_settings(buffer):\n",
    "    p1_x_str = ''\n",
    "    p2_x_str = ''\n",
    "    p_y_str = ''\n",
    "    length_str = ''\n",
    "    control = 0\n",
    "    for i in buffer:\n",
    "        if (control == 0):\n",
    "            if (i==';'):\n",
    "                control +=1\n",
    "            else:\n",
    "                p1_x_str += i\n",
    "        elif (control == 1):\n",
    "            if (i==';'):\n",
    "                control +=1\n",
    "            else:\n",
    "                p2_x_str += i\n",
    "        elif (control == 2):\n",
    "            if (i==';'):\n",
    "                control +=1\n",
    "            else:\n",
    "                p_y_str += i\n",
    "        elif (control == 3):\n",
    "            length_str += i\n",
    "    return(int(p1_x_str), int(p2_x_str), int(p_y_str), int(length_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGLAS: NADA DE PONERSE A ENTRENAR EN LA NOCHE ANTES DE QUE SE ACABE EL MES!!!\n",
    "#SI LO HACES TE DOY UNA PALIZA\n",
    "#Y SI YA TE PONES A HACERLO EN AÑO NUEVO TE MATO\n",
    "def setTime():\n",
    "    dt = datetime.datetime.now()\n",
    "    if (dt.second % 10 > 5):\n",
    "        if (dt.second>54): #Hay que pasar al siguiente minuto\n",
    "            if (dt.minute==59): #Hay que pasar a la siguiente hora\n",
    "                if (dt.hour == 23): #Hay que pasar al siguiente día\n",
    "                    dia = dt.day+1\n",
    "                    hora = 0\n",
    "                    minuto = 0\n",
    "                    sec = 5\n",
    "                    dt.replace(day=dia, hour=hora, minute=minuto, second=sec)\n",
    "                else: #hora nueva\n",
    "                    hora = dt.hour+1\n",
    "                    minuto = 0\n",
    "                    sec = 5\n",
    "                    dt.replace(hour=hora, minute=minuto, second=sec)\n",
    "            else: #minuto nuevo\n",
    "                minuto = dt.minute+1\n",
    "                sec = 5\n",
    "                dt = dt.replace(minute=minuto ,second=sec)\n",
    "        else: #caso \"simple\"\n",
    "            sec = (((dt.second // 10)+1)*10) + 5 #Z+1 5\n",
    "            dt = dt.replace(second=sec)\n",
    "    else:\n",
    "        if (dt.second>49): #Hay que pasar al siguiente minuto\n",
    "            if (dt.minute==59): #Hay que pasar a la siguiente hora\n",
    "                if (dt.hour == 23): #Hay que pasar al siguiente día\n",
    "                    dia = dt.day+1\n",
    "                    hora = 0\n",
    "                    minuto = 0\n",
    "                    sec = 5\n",
    "                    dt.replace(day=dia, hour=hora, minute=minuto, second=sec)\n",
    "                else: #hora nueva\n",
    "                    hora = dt.hour+1\n",
    "                    minuto = 0\n",
    "                    sec = 0\n",
    "                    dt.replace(hour=hora, minute=minuto, second=sec)\n",
    "            else: #minuto nuevo\n",
    "                minuto = dt.minute+1\n",
    "                sec = 0\n",
    "                dt = dt.replace(minute=minuto ,second=sec)\n",
    "        else: #caso \"simple\"\n",
    "            sec = ((dt.second // 10)+1)*10 #Z+1 0\n",
    "            dt = dt.replace(second=sec)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMicrosecs(microsecs):\n",
    "    fulldate = datetime.datetime.now()\n",
    "    fulldate = fulldate + datetime.timedelta(microseconds=microsecs)\n",
    "    return fulldate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMinute():\n",
    "    fulldate = datetime.datetime.now()\n",
    "    fulldate = fulldate + datetime.timedelta(minutes=1)\n",
    "    return fulldate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recvall(sock, count):\n",
    "    buf = b''\n",
    "    while count:\n",
    "        newbuf = sock.recv(count)\n",
    "        if not newbuf: return None\n",
    "        buf += newbuf\n",
    "        count -= len(newbuf)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The connection aspect of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_to_Monty(mensaje, port):\n",
    "    PORT = port\n",
    "    dir = \"localhost\"\n",
    "    dir_serv = (dir, PORT)\n",
    "    if (mensaje == \"get\"): #GET image\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((dir,PORT))\n",
    "        s.sendall(mensaje.encode())\n",
    "        w = int(recvall(s,8))\n",
    "        h = int(recvall(s,8))\n",
    "        length = recvall(s,16)\n",
    "        stringData = recvall(s, int(length))\n",
    "        data = numpy.fromstring(stringData, dtype='uint8').reshape(h,w,3)\n",
    "        im = Image.fromarray(data)\n",
    "        s.close()\n",
    "        return im\n",
    "    elif (mensaje == \"set\"): #We set the initial values of padding needed, health pos\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((dir,PORT))\n",
    "        s.sendall(mensaje.encode())\n",
    "        buf = s.recv(1024) #We will just need the p1_x, p2_x and y of setPadding()\n",
    "        #####(x_pad, y_pad, p1_x, p2_x, y, length) = setPadding()#####\n",
    "        print(buf.decode())\n",
    "        s.close()\n",
    "        (p1_x, p2_x, p_y, length) = translate_settings(buf.decode())\n",
    "        coordinate_p1 = x, y = p1_x, p_y\n",
    "        coordinate_p2 = x, y = p2_x, p_y\n",
    "        return (coordinate_p1, coordinate_p2, length)\n",
    "    elif (mensaje == \"start1\"): #START env and get wait time\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((dir,PORT))\n",
    "        s.sendall(mensaje.encode())\n",
    "        buf = s.recv(1024)\n",
    "        s.close()\n",
    "        converted = buf.decode()\n",
    "        if (converted == 'Yes                 '):\n",
    "            date = setTime()\n",
    "            now = datetime.datetime.now()\n",
    "            while(now<=date):\n",
    "                time.sleep(1)\n",
    "                now = datetime.datetime.now()\n",
    "        return converted\n",
    "    elif (mensaje[0] == \"1\" ): #SEND COMMAND\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((dir,PORT))\n",
    "        s.sendall(mensaje.encode())\n",
    "        s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starter_sync(num):\n",
    "    txt = connection_to_Monty(\"start1\",num)\n",
    "    while(txt == 'No                  '):\n",
    "        txt = connection_to_Monty(\"start1\",num)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \n",
    "           \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \n",
    "           \"10\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\",\n",
    "           \"20\", \"21\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\",\n",
    "           \"30\", \"31\", \"32\", \"34\", \"35\", \"36\", \"37\", \"38\",\n",
    "           \"40\", \"41\", \"42\", \"43\", \"45\", \"46\", \"47\", \"48\",\n",
    "           \"50\", \"51\", \"52\", \"53\", \"54\", \"56\", \"57\", \"58\",\n",
    "           \"60\", \"61\", \"62\", \"63\", \"64\", \"65\", \"67\", \"68\", \n",
    "           \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"78\",\n",
    "           \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algunos parámetros generales\n",
    "im_height = 394 #Full size\n",
    "im_width = 599 #Full size\n",
    "im_size_sides = im_height * im_width #Full size\n",
    "im_size = im_size_sides * 3 #la anchura y altura de la imagen junto con los canales rgb #Full size\n",
    "\n",
    "im_height_half = 197\n",
    "im_width_half = 300\n",
    "im_size_sides_half = im_height_half * im_width_half\n",
    "im_size_half = im_size_sides_half * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos la red del agente (Actor-Critic Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self,s_size,a_size,scope,trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.inputs = tf.placeholder(shape=[None, None,s_size],dtype=tf.float32)\n",
    "            self.imageIn = tf.reshape(self.inputs,shape=[-1,im_height_half,im_width_half,3])\n",
    "            self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.imageIn,num_outputs=16,\n",
    "                kernel_size=[8,8],stride=[4,4],padding='VALID')\n",
    "            self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.conv1,num_outputs=32,\n",
    "                kernel_size=[4,4],stride=[2,2],padding='VALID')\n",
    "            hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu)\n",
    "            \n",
    "            #Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(256,state_is_tuple=True)\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(self.imageIn)[:1]\n",
    "            state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n",
    "            \n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                weights_initializer=normalized_columns_initializer(0.01),\n",
    "                biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out,1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=normalized_columns_initializer(1.0),\n",
    "                biases_initializer=None)\n",
    "            \n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global_1_':\n",
    "                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "                self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n",
    "                \n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_movements(actionBuf):\n",
    "    total = 0\n",
    "    divisor = 0\n",
    "    for x in actionBuf:\n",
    "        divisor += 1\n",
    "        check = True\n",
    "        for y in x:\n",
    "            if (int(y)<4) and (check):\n",
    "                check = False\n",
    "                total +=1\n",
    "    return total/divisor\n",
    "\n",
    "def count_blocks(actionBuf):\n",
    "    total = 0\n",
    "    divisor = 0\n",
    "    for x in actionBuf:\n",
    "        divisor += 1\n",
    "        for y in x:\n",
    "            if y == '4':\n",
    "                total += 1\n",
    "    return total/divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_p1(status, actionBuff, current_length, hp1, hp2, flavour=\"vanilla\"):    \n",
    "    if (flavour == \"vanilla\"):\n",
    "        #Simple reward: IF WIN = 1 ; IF LOOSE = -1 ; IF DRAW = (hp2-hp1)/200 ; 200 == total health\n",
    "        #The other additions are in case we want to make a more complex \n",
    "        # FOR PLAYER TWO: 2 = draw ; 1 = WIN ; -1 = LOOSE (coincidentally the rewards coincide with the status, minus the draw)\n",
    "        if status == 2:\n",
    "            return (hp1-hp2)/200\n",
    "        elif status == 0:\n",
    "            return((hp1-hp2)/200)/100 #ENCOURAGEMENT\n",
    "        else:\n",
    "            return status\n",
    "    elif (flavour == \"aggro\"):\n",
    "        if status == 2:\n",
    "            return (hp1-hp2)/200\n",
    "        elif status == 1:\n",
    "            return (status * (68 / current_length))\n",
    "        elif status == 0:\n",
    "            return((hp1-hp2)/200)/100\n",
    "        else:\n",
    "            return status\n",
    "    elif (flavour == \"kite\"):\n",
    "        movements = count_movements(actionBuff)\n",
    "        if status == 2:\n",
    "            return ((hp1-hp2)/200)*movements\n",
    "        elif status == 1:\n",
    "            return status*movements\n",
    "        elif status == 0:\n",
    "            return((hp1-hp2)/200)/100\n",
    "        else:\n",
    "            return status\n",
    "    elif (flavour == \"tank\"):\n",
    "        blocks = count_blocks(actionBuff)\n",
    "        if status == 2:\n",
    "            return ((hp1-hp2)/200)*blocks\n",
    "        elif status == 1:\n",
    "            return status*blocks\n",
    "        elif status == 0:\n",
    "            return((hp1-hp2)/200)/100\n",
    "        else:\n",
    "            return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,name,port,s_size,a_size,trainer,model_path,global_episodes):\n",
    "        self.name = \"worker_1_\" + str(name)\n",
    "        self.num = name\n",
    "        self.player = 1\n",
    "        self.port = port\n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_1_\"+str(self.num))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(s_size,a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global_1_',self.name)        \n",
    "        \n",
    "        self.actions = self.actions = np.identity(a_size,dtype=bool).tolist()\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        observations = rollout[:,0]\n",
    "        actions = rollout[:,1]\n",
    "        rewards = rollout[:,2]\n",
    "        next_observations = rollout[:,3]\n",
    "        values = rollout[:,5]\n",
    "        \n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.inputs:[np.vstack(observations)],\n",
    "            self.local_AC.actions:actions,\n",
    "            self.local_AC.advantages:advantages,\n",
    "            self.local_AC.state_in[0]:self.batch_rnn_state[0],\n",
    "            self.local_AC.state_in[1]:self.batch_rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n, self.batch_rnn_state,_ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.state_out,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "        \n",
    "    def compete(self,max_eps,sess,coord_p1, coord_p2, lengt):\n",
    "        episode_count = 0\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.num))\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while (episode_count < max_eps):\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = 0\n",
    "                margin = 0\n",
    "                \n",
    "                starter_sync(self.port)\n",
    "                s = connection_to_Monty(\"get\", self.port)\n",
    "                wpercent = (im_width_half / float(s.size[0]))\n",
    "                reduced_s = s.resize((im_width_half, im_height_half), Image.ANTIALIAS)\n",
    "                s = np.reshape(np.array(reduced_s), (-1, im_size_half)) #HALF SIZE\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                self.batch_rnn_state = rnn_state\n",
    "                T1 = datetime.datetime.now()\n",
    "                while (d == 0):\n",
    "                    a_dist,v,rnn_state = sess.run([self.local_AC.policy,self.local_AC.value,self.local_AC.state_out], \n",
    "                        feed_dict={self.local_AC.inputs:[s],\n",
    "                        self.local_AC.state_in[0]:rnn_state[0],\n",
    "                        self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "\n",
    "                    connection_to_Monty(\"1\"+actions[a-1], self.port)\n",
    "                    s1 = connection_to_Monty(\"get\", self.port)\n",
    "                    (HP_1, HP_2) = getHealths(s1, coordinate_p1, coordinate_p2, length)\n",
    "                    margin = abs(HP_1-HP_2)/200\n",
    "                    d = is_game_over_neutral_endless(HP_1, HP_2)\n",
    "                    s1_reduced = reduced_s = s1.resize((im_width_half, im_height_half), Image.ANTIALIAS)\n",
    "                    s1 = np.reshape(np.array(s1_reduced), (-1, im_size_half)) #HALF SIZE\n",
    "\n",
    "                    s = s1                    \n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                T2 = datetime.datetime.now()\n",
    "                print(\"Episode: \" + str(episode_count)+\" ; LOOSER: \" + str(d) + \" ; MARGIN: \" + str(margin) + \" Time: \" + str(T2-T1))\n",
    "                if self.name == 'worker_1_1':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "    \n",
    "    def work(self,chosen_flavour,max_episode_length,max_eps,gamma,sess,coord,saver):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.num))\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while (episode_count <= max_eps):\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                action_buffer = []\n",
    "                d = 0\n",
    "                \n",
    "                starter_sync(self.port)\n",
    "                s = connection_to_Monty(\"get\", self.port)\n",
    "                wpercent = (im_width_half / float(s.size[0]))\n",
    "                reduced_s = s.resize((im_width_half, im_height_half), Image.ANTIALIAS)\n",
    "                s = np.reshape(np.array(reduced_s), (-1, im_size_half)) #HALF SIZE\n",
    "                episode_frames.append(s)\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                self.batch_rnn_state = rnn_state\n",
    "                time_limit = addMinute()\n",
    "                while (datetime.datetime.now() < time_limit):\n",
    "                    time_next_step = addMicrosecs(1000000/actions_per_second)\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state = sess.run([self.local_AC.policy,self.local_AC.value,self.local_AC.state_out], \n",
    "                        feed_dict={self.local_AC.inputs:[s],\n",
    "                        self.local_AC.state_in[0]:rnn_state[0],\n",
    "                        self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "\n",
    "                    connection_to_Monty(\"1\"+actions[a-1], self.port)\n",
    "                    action_buffer.append(actions[a-1])\n",
    "                    s1 = connection_to_Monty(\"get\", self.port)\n",
    "                    (HP_1, HP_2) = getHealths(s1, coordinate_p1[self.num-1], coordinate_p2[self.num-1], length[self.num-1])\n",
    "                    d = is_game_over(HP_1, HP_2, time_limit)\n",
    "                    r = calculate_reward_p1(d, action_buffer, episode_step_count, HP_1, HP_2, flavour = chosen_flavour)\n",
    "                    s1_reduced = reduced_s = s1.resize((im_width_half, im_height_half), Image.ANTIALIAS)\n",
    "                    s1 = np.reshape(np.array(s1_reduced), (-1, im_size_half)) #HALF SIZE\n",
    "\n",
    "                    if d == 0:\n",
    "                        episode_frames.append(s1)\n",
    "                    else:\n",
    "                        s1 = s\n",
    "                        \n",
    "                    if (d == 0):\n",
    "                        episode_buffer.append([s,a,r,s1,False,v[0,0]])\n",
    "                    else:\n",
    "                        episode_buffer.append([s,a,r,s1,True,v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "\n",
    "                    episode_reward += r\n",
    "                    s = s1                    \n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    \n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "                    if len(episode_buffer) == 30 and d == 0 and episode_step_count != max_episode_length - 1:\n",
    "                        # Since we don't know what the true final return is, we \"bootstrap\" from our current\n",
    "                        # value estimation.\n",
    "                        v1 = sess.run(self.local_AC.value, \n",
    "                            feed_dict={self.local_AC.inputs:[s],\n",
    "                            self.local_AC.state_in[0]:rnn_state[0],\n",
    "                            self.local_AC.state_in[1]:rnn_state[1]})[0,0]\n",
    "                        v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,v1)\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "                    if d != 0:\n",
    "                        break\n",
    "                    while(datetime.datetime.now()<time_next_step):\n",
    "                        time.sleep(5.0/1000.0)\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "                    print(\"Episode: \" + str(episode_count) + \"; P\" + str(self.player) +\" ; Steps of Episode: \" + str(episode_step_count) + \" ; Total reward: \" + str(episode_reward))   \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 5 == 0 and episode_count != 0:\n",
    "                    if episode_count % 100 == 0 and self.name == 'worker_1_1':\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print (\"Saved Model\")\n",
    "\n",
    "                    mean_reward = np.mean(self.episode_rewards[-5:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-5:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-5:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                    summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                    summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                    summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_1_1':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos de la sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_per_second = 2.5\n",
    "max_episode_length = actions_per_second * 60 #300\n",
    "max_eps = 700\n",
    "gamma = .99 # discount rate for advantage estimation and reward discounting\n",
    "s_size = im_size_half\n",
    "a_size = len(actions)\n",
    "load_model = True\n",
    "model_path = './model_1'\n",
    "#model_path = './model_1_aggro'\n",
    "#model_path = './model_1_kite'\n",
    "#model_path = './model_1_tank'\n",
    "flavour = \"vanilla\"\n",
    "#flavour = \"aggro\"\n",
    "#flavour = \"kite\"\n",
    "#flavour = \"tank\"\n",
    "PORT = 50010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(coordinate_p1_1, coordinate_p2_1, length_1) = connection_to_Monty(\"set\", PORT)\n",
    "(coordinate_p1_2, coordinate_p2_2, length_2) = connection_to_Monty(\"set\", PORT+2)\n",
    "coordinate_p1 = [coordinate_p1_1, coordinate_p1_2]\n",
    "coordinate_p2 = [coordinate_p2_1, coordinate_p2_2]\n",
    "length = [length_1, length_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-02 14:34:29.516137\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes_1',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = AC_Network(s_size,a_size,'global_1_',None) # Generate global network\n",
    "    num_workers = int(multiprocessing.cpu_count()/2)-2 # Set workers to number of available CPU threads / 2, for the other half\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(i+1,PORT+(i*2),s_size,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # Start the \"work\" process for each worker in a separate threat.\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(flavour,max_episode_length,max_eps,gamma,sess,coord,saver)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        time.sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)\n",
    "    \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_per_second = 2.5\n",
    "max_episode_length = actions_per_second * 60 #300\n",
    "work_eps = 25\n",
    "gamma = .99 # discount rate for advantage estimation and reward discounting\n",
    "s_size = im_size_half\n",
    "a_size = len(actions)\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT = 50005\n",
    "(coordinate_p1, coordinate_p2, length) = connection_to_Monty(\"set\", PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_1'\n",
    "#model_path = './model_1_aggro'\n",
    "#model_path = './model_1_kite'\n",
    "#model_path = './model_1_tank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes_1',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    master_network = AC_Network(s_size,a_size,'global_1_',None) # Generate global network\n",
    "    num_workers = 1 # Set workers to number of available CPU threads / 2, for the other half\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(i+1,PORT+(i*2),s_size,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.compete(work_eps,sess,coordinate_p1,coordinate_p2,length)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        time.sleep(0.5)\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
