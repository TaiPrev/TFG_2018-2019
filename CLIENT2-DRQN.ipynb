{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE CLIENT PART IS GOING TO RECEIVE ONLY A FRAGMENT OF THE IMAGE, \n",
    "#THE FRAGMENT CONTAINING THE ACTUAL PLAY AREA\n",
    "#BECAUSE OF THIS WE'LL ONLY NEED:\n",
    "#GET HEALTH (and it's necesary ops)\n",
    "#GET PLAYERS POS (and it's necesary ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime, time\n",
    "import socket, sys, cv2, numpy, time\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import csv\n",
    "import datetime, time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isGreen(rgb):\n",
    "    r, g, b = rgb\n",
    "    if (g>r) and (g>b):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isBlue(rgb):\n",
    "    r, g, b = rgb\n",
    "    if (b>r) and (b>g):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isRed(rgb):\n",
    "    r, g, b = rgb\n",
    "    if (r>b) and (r>g):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHealths(im, coord_p1, coord_p2, right_end):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    #verde es #3CD400 == rgb(60,212,0) aproximadamente, isGreen comprueba que G sea el valor principal\n",
    "    for x in range(right_end+1):\n",
    "        if (isGreen(im.getpixel(coord_p1))):\n",
    "            p1 = p1 + 1\n",
    "        if (isGreen(im.getpixel(coord_p2))):\n",
    "            p2 = p2 + 1\n",
    "        coord_p1 = x, y = coord_p1[0]+1, coord_p1[1]\n",
    "        coord_p2 = x, y = coord_p2[0]+1, coord_p2[1]\n",
    "    return(p1,p2)\n",
    "\n",
    "def is_game_over(HP_1, HP_2, end_time):\n",
    "    if (HP_1 == 0):\n",
    "        return 1 #P1 LOOSES\n",
    "    elif (HP_2 == 0):\n",
    "        return -1 #P2 LOOSES\n",
    "    elif ((HP_1 == 0) and (HP_2 == 0)) or (datetime.datetime.now() > end_time):\n",
    "        return 2 #DRAW\n",
    "    return 0\n",
    "\n",
    "def is_game_over_neutral_endless(HP_1, HP_2):\n",
    "    if (HP_1 == 0):\n",
    "        return 1 #P1 LOOSES\n",
    "    elif (HP_2 == 0):\n",
    "        return 2 #P2 LOOSES\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_settings(buffer):\n",
    "    p1_x_str = ''\n",
    "    p2_x_str = ''\n",
    "    p_y_str = ''\n",
    "    length_str = ''\n",
    "    control = 0\n",
    "    for i in buffer:\n",
    "        if (control == 0):\n",
    "            if (i==';'):\n",
    "                control +=1\n",
    "            else:\n",
    "                p1_x_str += i\n",
    "        elif (control == 1):\n",
    "            if (i==';'):\n",
    "                control +=1\n",
    "            else:\n",
    "                p2_x_str += i\n",
    "        elif (control == 2):\n",
    "            if (i==';'):\n",
    "                control +=1\n",
    "            else:\n",
    "                p_y_str += i\n",
    "        elif (control == 3):\n",
    "            length_str += i\n",
    "    return(int(p1_x_str), int(p2_x_str), int(p_y_str), int(length_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGLAS: NADA DE PONERSE A ENTRENAR EN LA NOCHE ANTES DE QUE SE ACABE EL MES!!!\n",
    "#SI LO HACES TE DOY UNA PALIZA\n",
    "#Y SI YA TE PONES A HACERLO EN AÑO NUEVO TE MATO\n",
    "def setTime():\n",
    "    dt = datetime.datetime.now()\n",
    "    if (dt.second % 10 > 5):\n",
    "        if (dt.second>54): #Hay que pasar al siguiente minuto\n",
    "            if (dt.minute==59): #Hay que pasar a la siguiente hora\n",
    "                if (dt.hour == 23): #Hay que pasar al siguiente día\n",
    "                    dia = dt.day+1\n",
    "                    hora = 0\n",
    "                    minuto = 0\n",
    "                    sec = 5\n",
    "                    dt.replace(day=dia, hour=hora, minute=minuto, second=sec)\n",
    "                else: #hora nueva\n",
    "                    hora = dt.hour+1\n",
    "                    minuto = 0\n",
    "                    sec = 5\n",
    "                    dt.replace(hour=hora, minute=minuto, second=sec)\n",
    "            else: #minuto nuevo\n",
    "                minuto = dt.minute+1\n",
    "                sec = 5\n",
    "                dt = dt.replace(minute=minuto ,second=sec)\n",
    "        else: #caso \"simple\"\n",
    "            sec = (((dt.second // 10)+1)*10) + 5 #Z+1 5\n",
    "            dt = dt.replace(second=sec)\n",
    "    else:\n",
    "        if (dt.second>49): #Hay que pasar al siguiente minuto\n",
    "            if (dt.minute==59): #Hay que pasar a la siguiente hora\n",
    "                if (dt.hour == 23): #Hay que pasar al siguiente día\n",
    "                    dia = dt.day+1\n",
    "                    hora = 0\n",
    "                    minuto = 0\n",
    "                    sec = 5\n",
    "                    dt.replace(day=dia, hour=hora, minute=minuto, second=sec)\n",
    "                else: #hora nueva\n",
    "                    hora = dt.hour+1\n",
    "                    minuto = 0\n",
    "                    sec = 0\n",
    "                    dt.replace(hour=hora, minute=minuto, second=sec)\n",
    "            else: #minuto nuevo\n",
    "                minuto = dt.minute+1\n",
    "                sec = 0\n",
    "                dt = dt.replace(minute=minuto ,second=sec)\n",
    "        else: #caso \"simple\"\n",
    "            sec = ((dt.second // 10)+1)*10 #Z+1 0\n",
    "            dt = dt.replace(second=sec)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMicrosecs(microsecs):\n",
    "    fulldate = datetime.datetime.now()\n",
    "    fulldate = fulldate + datetime.timedelta(microseconds=microsecs)\n",
    "    return fulldate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMinute():\n",
    "    fulldate = datetime.datetime.now()\n",
    "    fulldate = fulldate + datetime.timedelta(minutes=1)\n",
    "    return fulldate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recvall(sock, count):\n",
    "    buf = b''\n",
    "    while count:\n",
    "        newbuf = sock.recv(count)\n",
    "        if not newbuf: return None\n",
    "        buf += newbuf\n",
    "        count -= len(newbuf)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The connection aspect of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_to_Monty(mensaje):\n",
    "    PORT = 50006\n",
    "    dir = \"localhost\"\n",
    "    if (mensaje == \"get\"): #GET image\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((dir,PORT))\n",
    "        s.sendall(mensaje.encode())\n",
    "        w = int(recvall(s,8))\n",
    "        h = int(recvall(s,8))\n",
    "        length = recvall(s,16)\n",
    "        stringData = recvall(s, int(length))\n",
    "        data = numpy.fromstring(stringData, dtype='uint8').reshape(h,w,3)\n",
    "        im = Image.fromarray(data)\n",
    "        s.close()\n",
    "        return im\n",
    "    elif (mensaje == \"set\"): #We set the initial values of padding needed, health pos\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((dir,PORT))\n",
    "        s.sendall(mensaje.encode())\n",
    "        buf = s.recv(1024) #We will just need the p1_x, p2_x and y of setPadding()\n",
    "        #####(x_pad, y_pad, p1_x, p2_x, y, length) = setPadding()#####\n",
    "        print(buf.decode())\n",
    "        s.close()\n",
    "        (p1_x, p2_x, p_y, length) = translate_settings(buf.decode())\n",
    "        coordinate_p1 = x, y = p1_x, p_y\n",
    "        coordinate_p2 = x, y = p2_x, p_y\n",
    "        return (coordinate_p1, coordinate_p2, length)\n",
    "    elif (mensaje == \"start2\"): #START env and get wait time\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((dir,(PORT-1)))\n",
    "        s.sendall(mensaje.encode())\n",
    "        buf = s.recv(1024)\n",
    "        s.close()\n",
    "        converted = buf.decode()\n",
    "        if (converted == 'Yes                 '):\n",
    "            date = setTime()\n",
    "            now = datetime.datetime.now()\n",
    "            while(now<=date):\n",
    "                time.sleep(1)\n",
    "                now = datetime.datetime.now()\n",
    "        return converted\n",
    "    elif (mensaje[0] == \"2\" ): #SEND COMMAND\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((dir,PORT))\n",
    "        s.sendall(mensaje.encode())\n",
    "        s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starter_sync():\n",
    "    txt = connection_to_Monty(\"start2\")\n",
    "    while(txt == 'No                  '):\n",
    "        txt = connection_to_Monty(\"start2\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \n",
    "           \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \n",
    "           \"10\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\",\n",
    "           \"20\", \"21\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\",\n",
    "           \"30\", \"31\", \"32\", \"34\", \"35\", \"36\", \"37\", \"38\",\n",
    "           \"40\", \"41\", \"42\", \"43\", \"45\", \"46\", \"47\", \"48\",\n",
    "           \"50\", \"51\", \"52\", \"53\", \"54\", \"56\", \"57\", \"58\",\n",
    "           \"60\", \"61\", \"62\", \"63\", \"64\", \"65\", \"67\", \"68\", \n",
    "           \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"78\",\n",
    "           \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\"]\n",
    "num_actions = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Algunos parámetros generales\n",
    "im_height = 394 #Full size\n",
    "im_width = 599 #Full size\n",
    "im_size_sides = im_height * im_width #Full size\n",
    "im_size = im_size_sides * 3 #la anchura y altura de la imagen junto con los canales rgb #Full size\n",
    "\n",
    "im_height_half = 197\n",
    "im_width_half = 300\n",
    "im_size_sides_half = im_height_half * im_width_half\n",
    "im_size_half = im_size_sides_half * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos la red del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,rnn_cell,myScope, size=1):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        if (size == 1):\n",
    "            self.scalarInput =  tf.placeholder(shape=[None,None,im_size],dtype=tf.float32) #FULL SIZE\n",
    "            self.imageIn = tf.reshape(self.scalarInput,shape=[-1,im_height,im_width,3]) #FULL SIZE\n",
    "            self.conv1 = slim.convolution2d( \\\n",
    "                inputs=self.imageIn,num_outputs=32,\\\n",
    "                kernel_size=[18,18],stride=[8,8],padding='VALID', \\\n",
    "                biases_initializer=None,scope=myScope+'_conv1')\n",
    "            self.conv2 = slim.convolution2d( \\\n",
    "                inputs=self.conv1,num_outputs=64,\\\n",
    "                kernel_size=[10,10],stride=[4,4],padding='VALID', \\\n",
    "                biases_initializer=None,scope=myScope+'_conv2')\n",
    "            self.conv3 = slim.convolution2d( \\\n",
    "                inputs=self.conv2,num_outputs=64,\\\n",
    "                kernel_size=[6,6],stride=[2,2],padding='VALID', \\\n",
    "                biases_initializer=None,scope=myScope+'_conv3')\n",
    "            self.conv4 = slim.convolution2d( \\\n",
    "                inputs=self.conv3,num_outputs=h_size,\\\n",
    "                kernel_size=[3,3],stride=[6,6],padding='VALID', \\\n",
    "                biases_initializer=None,scope=myScope+'_conv4')\n",
    "        \n",
    "            self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "            #We take the output from the final convolutional layer and send it to a recurrent layer.\n",
    "            #The input must be reshaped into [batch x trace x units] for rnn processing, \n",
    "            #and then returned to [batch x units] when sent through the upper levels.\n",
    "            self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "            self.convFlat = tf.reshape(self.conv4, [self.batch_size,self.trainLength,h_size])\n",
    "            #For the previous RNN_CELL\n",
    "            self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "            #We create the RNN and it's initial state\n",
    "            self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                cell=rnn_cell,inputs=self.convFlat,initial_state=self.state_in,dtype=tf.float32,scope=myScope+'_rnn')\n",
    "            self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "            #The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "            self.streamA,self.streamV = tf.split(self.rnn,2,1) #stream of Advantage, stream of Value\n",
    "            self.AW = tf.Variable(tf.random_normal([h_size//2,len(actions)])) #Advantage weight/worth\n",
    "            self.VW = tf.Variable(tf.random_normal([h_size//2,1])) #Value weight/worth\n",
    "            self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "            self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "            self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "            #Then combine them together to get our final Q-values.\n",
    "            self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "            self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "            #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "            self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,num_actions,dtype=tf.float32)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "            #In order to only propogate accurate gradients through the network, we will mask the first\n",
    "            #half of the losses for each trace as per Lample & Chatlot 2016\n",
    "            self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "            self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "            self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "            self.mask = tf.reshape(self.mask,[-1])\n",
    "            self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "            \n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)\n",
    "        #END IF  \n",
    "        \n",
    "        elif (size == 0.5):    \n",
    "            self.scalarInput =  tf.placeholder(shape=[None,None,im_size_half],dtype=tf.float32) #FULL SIZE\n",
    "            self.imageIn = tf.reshape(self.scalarInput,shape=[-1,im_height_half,im_width_half,3]) #FULL SIZE\n",
    "            self.conv1 = slim.convolution2d( \\\n",
    "                inputs=self.imageIn,num_outputs=32,\\\n",
    "                kernel_size=[14,14],stride=[6,6],padding='VALID', \\\n",
    "                biases_initializer=None,scope=myScope+'_conv1')\n",
    "            self.conv2 = slim.convolution2d( \\\n",
    "                inputs=self.conv1,num_outputs=64,\\\n",
    "                kernel_size=[10,10],stride=[3,3],padding='VALID', \\\n",
    "                biases_initializer=None,scope=myScope+'_conv2')\n",
    "            self.conv3 = slim.convolution2d( \\\n",
    "                inputs=self.conv2,num_outputs=64,\\\n",
    "                kernel_size=[6,6],stride=[1,1],padding='VALID', \\\n",
    "                biases_initializer=None,scope=myScope+'_conv3')\n",
    "            self.conv4 = slim.convolution2d( \\\n",
    "                inputs=self.conv3,num_outputs=h_size,\\\n",
    "                kernel_size=[3,3],stride=[6,6],padding='VALID', \\\n",
    "                biases_initializer=None,scope=myScope+'_conv4')\n",
    "        \n",
    "            self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "            #We take the output from the final convolutional layer and send it to a recurrent layer.\n",
    "            #The input must be reshaped into [batch x trace x units] for rnn processing, \n",
    "            #and then returned to [batch x units] when sent through the upper levels.\n",
    "            self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "            self.convFlat = tf.reshape(self.conv4, [self.batch_size,self.trainLength,h_size])\n",
    "            #For the previous RNN_CELL\n",
    "            self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "            #We create the RNN and it's initial state\n",
    "            self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                cell=rnn_cell,inputs=self.convFlat,initial_state=self.state_in,dtype=tf.float32,scope=myScope+'_rnn')\n",
    "            self.rnn = tf.reshape(self.rnn,shape=[-1,h_size])\n",
    "            #The output from the recurrent player is then split into separate Value and Advantage streams\n",
    "            self.streamA,self.streamV = tf.split(self.rnn,2,1) #stream of Advantage, stream of Value\n",
    "            self.AW = tf.Variable(tf.random_normal([h_size//2,len(actions)])) #Advantage weight/worth\n",
    "            self.VW = tf.Variable(tf.random_normal([h_size//2,1])) #Value weight/worth\n",
    "            self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "            self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "            self.salience = tf.gradients(self.Advantage,self.imageIn)\n",
    "            #Then combine them together to get our final Q-values.\n",
    "            self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "            self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "            #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "            self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,num_actions,dtype=tf.float32)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "            #In order to only propogate accurate gradients through the network, we will mask the first\n",
    "            #half of the losses for each trace as per Lample & Chatlot 2016\n",
    "            self.maskA = tf.zeros([self.batch_size,self.trainLength//2])\n",
    "            self.maskB = tf.ones([self.batch_size,self.trainLength//2])\n",
    "            self.mask = tf.concat([self.maskA,self.maskB],1)\n",
    "            self.mask = tf.reshape(self.mask,[-1])\n",
    "            self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "            \n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)\n",
    "        #END IF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 100):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer,batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A bit of added help\n",
    "def updateTargetGraph(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "    total_vars = len(tf.trainable_variables())\n",
    "    a = tf.trainable_variables()[0].eval(session=sess)\n",
    "    b = tf.trainable_variables()[total_vars//2].eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_movements(actionBuf):\n",
    "    total = 0\n",
    "    divisor = 0\n",
    "    for x in actionBuf:\n",
    "        divisor += 1\n",
    "        check = True\n",
    "        if (int(x[0])<4) and (check):\n",
    "                check = False\n",
    "                total +=1\n",
    "    return total/divisor\n",
    "\n",
    "def count_blocks(actionBuf):\n",
    "    total = 0\n",
    "    divisor = 0\n",
    "    for x in actionBuf:\n",
    "        divisor += 1\n",
    "        if x[0] == '4':\n",
    "            total += 1\n",
    "    return total/divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward_p2(status, actionBuff, current_length, hp1, hp2, flavour=\"vanilla\"):\n",
    "    if (flavour == \"vanilla\"):\n",
    "        #Simple reward: IF WIN = 1 ; IF LOOSE = -1 ; IF DRAW = (hp2-hp1)/200 ; 200 == total health\n",
    "        #The other additions are in case we want to make a more complex \n",
    "        # FOR PLAYER TWO: 2 = draw ; 1 = WIN ; -1 = LOOSE (coincidentally the rewards coincide with the status, minus the draw)\n",
    "        if status == 2:\n",
    "            return (hp2-hp1)/200\n",
    "        elif status == 0:\n",
    "            return((hp2-hp1)/200)/100 #ENCOURAGEMENT\n",
    "        else:\n",
    "            return status\n",
    "    elif (flavour == \"aggro\"):\n",
    "        if status == 2:\n",
    "            return (hp2-hp1)/200\n",
    "        elif status == 1:\n",
    "            return (status * (68 / current_length))\n",
    "        elif status == 0:\n",
    "            return((hp2-hp1)/200)/100 #ENCOURAGEMENT\n",
    "        else:\n",
    "            return status\n",
    "    elif (flavour == \"kite\"):\n",
    "        movements = count_movements(actionBuff)\n",
    "        if status == 2:\n",
    "            return ((hp2-hp1)/200)*movements\n",
    "        elif status == 1:\n",
    "            return status*movements\n",
    "        elif status == 0:\n",
    "            return((hp2-hp1)/200)/100 #ENCOURAGEMENT\n",
    "        else:\n",
    "            return status\n",
    "    elif (flavour == \"tank\"):\n",
    "        blocks = count_blocks(actionBuff)\n",
    "        if status == 2:\n",
    "            return ((hp2-hp1)/200)*blocks\n",
    "        elif status == 1:\n",
    "            return status*blocks\n",
    "        elif status == 0:\n",
    "            return((hp2-hp1)/200)/100 #ENCOURAGEMENT\n",
    "        else:\n",
    "            return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecer datos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_per_second = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the training parameters\n",
    "batch_size = 4 #How many experience traces to use for each training step.\n",
    "trace_length = 8 #How long each experience trace will be when training\n",
    "update_freq = 5 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "max_epLength = 60 * actions_per_second #The max allowed length of our episode.\n",
    "anneling_steps = 200 * max_epLength #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 700 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 100 * max_epLength #How many steps of random actions before training begins.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./drqn_2_02\" #The path to save our model to.\n",
    "#path = \"./drqn_2_02_AGGRO\"\n",
    "#path = \"./drqn_2_02_KITE\"\n",
    "\n",
    "flavour = \"vanilla\"\n",
    "#flavour = \"aggro\"\n",
    "#flavour = \"kite\"\n",
    "\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "summaryLength = 100 #Number of epidoes to periodically save for analysis\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17;376;25;200       \n"
     ]
    }
   ],
   "source": [
    "(coordinate_p1, coordinate_p2, length) = connection_to_Monty(\"set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fase de entrenamiento del modelo de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#We define the cells for the primary and target q-networks\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main', size=0.5)\n",
    "targetQN = Qnetwork(h_size,cellT,'target', size=0.5)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        if (ckpt == None):\n",
    "            print('There is no model to load!')\n",
    "        else:\n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "   \n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to thagfslodfpkyfpesa primary network.\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = []\n",
    "        #Reset environment and get first new observation\n",
    "        actionBuffer = [] #Buffer of actions\n",
    "        starter_sync() #El proceso terminará a tiempo para que sea un inicio sincronizado\n",
    "        s = connection_to_Monty(\"get\")\n",
    "        #se reduce el tamaño de la imagen\n",
    "        wpercent = (im_width_half / float(s.size[0]))\n",
    "        reduced_s = s.resize((im_width_half, im_height_half), Image.ANTIALIAS)\n",
    "        s_array = np.reshape(np.array(reduced_s), (-1, im_size_half)) #HALF SIZE\n",
    "        d = 0 # while 0 -> The game continues\n",
    "        rAll = 0\n",
    "        j = 0 #Ep_length\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size])) #Reset the recurrent layer's hidden state\n",
    "        #The Q-Network\n",
    "        epEndTime = addMinute()\n",
    "        while j < max_epLength: \n",
    "            time_next_step = addMicrosecs(1000000/actions_per_second)\n",
    "            j+=1\n",
    "            (HP_1, HP_2) = getHealths(s, coordinate_p1, coordinate_p2, length)\n",
    "            d = is_game_over(HP_1, HP_2, epEndTime)\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:   \n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                        feed_dict={mainQN.scalarInput:[s_array],\\\n",
    "                        mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})                \n",
    "                a = np.random.randint(0,len(actions)) #from our defined actions\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s_array],\\\n",
    "                    mainQN.trainLength:1,mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            connection_to_Monty(\"2\"+actions[a-1])\n",
    "            actionBuffer.append(actions[a-1])\n",
    "            r = calculate_reward_p2(d, actionBuffer, j, HP_1, HP_2)\n",
    "            s1 = connection_to_Monty(\"get\")\n",
    "            s1_reduced = reduced_s = s1.resize((im_width_half, im_height_half), Image.ANTIALIAS)\n",
    "            s1_array = np.reshape(np.array(s1_reduced), (-1, im_size_half)) #HALF SIZE\n",
    "            total_steps += 1\n",
    "            if (d == 0):\n",
    "                episodeBuffer.append(np.reshape(np.array([s_array,a,r,s1_array,False]),[1,5]))\n",
    "            else:\n",
    "                episodeBuffer.append(np.reshape(np.array([s_array,a,r,s1_array,True]),[1,5]))\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    updateTarget(targetOps,sess)\n",
    "                    #Reset the recurrent layer's hidden state\n",
    "                    state_train = (np.zeros([batch_size,h_size]),np.zeros([batch_size,h_size])) \n",
    "                    \n",
    "                    trainBatch = myBuffer.sample(batch_size,trace_length) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={\\\n",
    "                        mainQN.scalarInput:[np.vstack(trainBatch[:,3])],\\\n",
    "                        mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={\\\n",
    "                        targetQN.scalarInput:[np.vstack(trainBatch[:,3])],\\\n",
    "                        targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:batch_size})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size*trace_length),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:[np.vstack(trainBatch[:,0])],mainQN.targetQ:targetQ,\\\n",
    "                        mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n",
    "                        mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n",
    "            rAll += r #total reward\n",
    "            s = s1\n",
    "            s_array = s1_array\n",
    "            #sP = s1P\n",
    "            state = state1\n",
    "            if d != 0: #probably take it out\n",
    "                break\n",
    "            while(datetime.datetime.now()<time_next_step):\n",
    "                time.sleep(5.0 / 1000.0)\n",
    "                \n",
    "        #Add the episode to the experience buffer\n",
    "        if (j > 8): #MINIMUM NUMBER OF STEPS PER EPISODE\n",
    "            bufferArray = np.array(episodeBuffer)\n",
    "            episodeBuffer = list(zip(bufferArray))\n",
    "            myBuffer.add(episodeBuffer)\n",
    "            jList.append(j)\n",
    "            rList.append(rAll)\n",
    "            if (rAll == 0) and (e < endE):\n",
    "                e += 0.2\n",
    "            print(\"Episode: \" + str(i) + \" ; Steps: \" + str(j) + \" ; Total reward: \" + str(rAll) + \" ; e = \" + str(e))\n",
    "            #Periodically save the model. \n",
    "            if i % 50 == 0 and i != 0:\n",
    "                saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "                print (\"Saved Model\")\n",
    "            if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "                print (total_steps,np.mean(rList[-summaryLength:]), e)\n",
    "        else:\n",
    "            i = i-1 #THE EPISODE IS UNDONE \n",
    "            print(\"Insuficient steps in episode.\")\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empleo de la red entrenada\n",
    "\n",
    "Se prueba el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_per_second = 2.4\n",
    "e = 0.01\n",
    "max_epLength = 60 * actions_per_second #The max allowed length of our episode.\n",
    "num_episodes = 25 #How many episodes of game environment to train network with.\n",
    "load_model = True #Whether to load a saved model.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./drqn_2_02\"\n",
    "#path = \"./drqn_2_02_AGGRO\"\n",
    "#path = \"./drqn_2_02_KITE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(coordinate_p1, coordinate_p2, length) = connection_to_Monty(\"set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#We define the cells for the primary and target q-networks\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size,cell,'main', size=0.5)\n",
    "targetQN = Qnetwork(h_size,cellT,'target', size=0.5)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "path2 = \"./Center2\"\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "\n",
    "        \n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        starter_sync() #El proceso terminará a tiempo para que sea un inicio sincronizado\n",
    "        s = connection_to_Monty(\"get\")\n",
    "        #se reduce el tamaño de la imagen\n",
    "        wpercent = (im_width_half / float(s.size[0]))\n",
    "        reduced_s = s.resize((im_width_half, im_height_half), Image.ANTIALIAS)\n",
    "        s_array = np.reshape(np.array(reduced_s), (-1, im_size_half)) #HALF SIZE\n",
    "        d = 0\n",
    "        #rAll = 0\n",
    "        j = 0\n",
    "        margin = 0\n",
    "        state = (np.zeros([1,h_size]),np.zeros([1,h_size]))\n",
    "        #The Q-Network\n",
    "        T1 = datetime.datetime.now()\n",
    "        while d == 0: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            time_next_step = addMicrosecs(1000000/actions_per_second)\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e:\n",
    "                state1 = sess.run(mainQN.rnn_state,\\\n",
    "                    feed_dict={mainQN.scalarInput:[s_array],\\\n",
    "                               mainQN.trainLength:1,mainQN.state_in:state,\\\n",
    "                               mainQN.batch_size:1})\n",
    "                a = np.random.randint(0,len(actions))\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict,mainQN.rnn_state],\\\n",
    "                    feed_dict={mainQN.scalarInput:[s_array],mainQN.trainLength:1,\\\n",
    "                    mainQN.state_in:state,mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            connection_to_Monty(\"2\"+actions[a-1]) #MIGHT need some helpt in adjusting this)\n",
    "            s1 = connection_to_Monty(\"get\")\n",
    "            (HP_1, HP_2) = getHealths(s1, coordinate_p1, coordinate_p2, length)\n",
    "            d = is_game_over_neutral_endless(HP_1, HP_2)\n",
    "            margin = abs(HP_1-HP_2)/200\n",
    "            s1_reduced = reduced_s = s1.resize((im_width_half, im_height_half), Image.ANTIALIAS)\n",
    "            s1_array = np.reshape(np.array(s1_reduced), (-1, im_size_half)) #HALF SIZE\n",
    "            total_steps += 1\n",
    "            s = s1\n",
    "            state = state1\n",
    "            while(datetime.datetime.now()<time_next_step):\n",
    "                time.sleep(5.0 / 1000.0)\n",
    "        \n",
    "        T2 = datetime.datetime.now()\n",
    "        print(\"Episode: \" + str(i) + \" ; LOOSER: \" + str(d) + \" ; MARGIN: \" + str(margin) + \" ; Time: \" + str(T2-T1))\n",
    "        #Periodically save the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
